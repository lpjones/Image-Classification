{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fcbbf9",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification\n",
    "Now that you have some basic familiarity with Pytorch and torch tensors, we can start to work with Pytorch to create an image classifier with a neural network. \n",
    "\n",
    "## Getting Started\n",
    "We first need to import all the necessary modules from Pytorch that we'll need to make our image classification neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc76898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd27b6",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters\n",
    "Hyperparameters are how you as an engineer control the performance of the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb939b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10 \n",
    "num_epochs = 2\n",
    "batch_size = 100 \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab16498",
   "metadata": {},
   "source": [
    "An explanation of each hyperparameter:\n",
    "\n",
    "\n",
    "*   ```input_size```: The size of the input data\n",
    "*   ```num_classes```: The number of output classes\n",
    "*   ```num_epochs```: The number of times the training data will be passed over\n",
    "*   ```batch_size```: The number of images we process at each training step\n",
    "*   ```lr```: The Learning Rate at which the optimizer updates each weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59309fc",
   "metadata": {},
   "source": [
    "## Downloading MNIST\n",
    "We will now download and load the MNIST dataset. We load data  to train on, as well as test data to track the model's performance. Here we use the dataset module from Pytorch in order to download and load MNIST. We also use the transform package to make everything Torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04e0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41308c1d7f7444d902fbcb1475647ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0189fdd2b95f48939327d26f613fb9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862dded313c54145b1610c3500c7e7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981d27cf049e465ea7f491081ada7d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554786078/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = datasets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb2fd0",
   "metadata": {},
   "source": [
    "Next we use a Pytorch dataloader so we can efficiently send batches of training data from our dataset to our model. The Pytorch dataloader is a memory efficient way to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7244f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52633d",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(utils.make_grid(images))\n",
    "print(' '.join('%d' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2cacd",
   "metadata": {},
   "source": [
    "## Define Network Architecture\n",
    "After loading the data, we now need to define our network architecture using the torch nn module. We do this by inheriting from the nn.Module class. Our initial basic model is very simple. We have one hidden layer with 500 neurons, connecting our input to our output. The activation function we use is ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33fe7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-Layer Neural Network called Net\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, 500)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(500, num_classes)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990b8d7",
   "metadata": {},
   "source": [
    "For each network you define you must specify the layers and order of each layers in the class in the constructor. You specify how the forward propagation works for each layer using the forward function. The backpropagation step will then be automatically done by Pytorch with order of execution specified as the reverse of the forward function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b6a03",
   "metadata": {},
   "source": [
    "## Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07ff256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(input_size, num_classes)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf777dba",
   "metadata": {},
   "source": [
    "\n",
    "## Define a Loss Function and Instantiate an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fb9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc750347",
   "metadata": {},
   "source": [
    "We define a loss function variable and then instantiate an optimizer according to our network parameters with the learning rate we specified in our hyperparameters. Here we use the ADAM optimizer instead of the gradient descent algorithm we studied in class. They both still accomplish the same thing for us, which is updating the weights of our network. If you'd like to know the details of ADAM and optimization in general, I encourage you to read through [this textbook chapter](https://d2l.ai/chapter_optimization/).\n",
    "\n",
    "## Start the Training Loop\n",
    "After loading our data, specifying the model architecture, defining a loss function, and specifying an optimizer, we can move to actually training the model. The following code demonstrates how to run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f4aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.2462\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2000\n",
      "Epoch [1/2], Step [300/600], Loss: 0.3208\n",
      "Epoch [1/2], Step [400/600], Loss: 0.2939\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1455\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0836\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0577\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0342\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0702\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0867\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0984\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1198\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all Epochs\n",
    "for epoch in range(num_epochs):\n",
    "  # Iterate through training dataset\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    # Flatten images and load images/labels\n",
    "    images, labels = data[0].cuda(), data[1].cuda()\n",
    "    images = images.view(-1, input_size)\n",
    "    # Zero collected gradients at each step\n",
    "    optimizer.zero_grad()\n",
    "    # Forward Propagate\n",
    "    outputs = net(images)\n",
    "    # Calculate Loss\n",
    "    loss = loss_function(outputs, labels)\n",
    "    # Back propagate\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print statistics on every 100th iteration\n",
    "    if (i+1) % 100 == 0:\n",
    "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, \n",
    "                   len(train_data)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472b58a",
   "metadata": {},
   "source": [
    "## Evaluate the Performance of your Model\n",
    "The following code snippet demonstrates how to evaluate the performane of the model's current weights using the test data we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcdf273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        images, labels = test_data[0].cuda(), test_data[1].cuda()\n",
    "        images = images.view(-1, input_size)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87690ac",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Below are two utility functions that wrap the training loop and test accuracy functions into singular methods. This should help you to more easily iterate over different hyperparameter configurations for the assignment. The second cell illustrates the usage of both functions with our initial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d4e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, optimizer, train_loader, batch_size, num_epochs, input_size, stat_count=100, device=None):\n",
    "    if device is not None:\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "    # Iterate through all Epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Iterate through training dataset\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Flatten images and load images/labels onto GPU\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            images = images.view(-1, input_size)\n",
    "            # Zero collected gradients at each step\n",
    "            optimizer.zero_grad()\n",
    "            # Forward Propagate\n",
    "            outputs = model(images)\n",
    "            # Calculate Loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Back propagate\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics on every stat_count iteration\n",
    "            if (i+1) % stat_count == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                            %(epoch+1, num_epochs, i+1, \n",
    "                            len(train_loader), loss.item()))\n",
    "\n",
    "def test_accuracy(model, test_loader, input_size, device=None):\n",
    "    if device is not None:\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data in test_loader:\n",
    "            images, labels = test_data[0].cuda(), test_data[1].cuda()\n",
    "            images = images.view(-1, input_size)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31309d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.3373\n",
      "Epoch [1/2], Step [200/600], Loss: 0.3298\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1599\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1604\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1186\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1885\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1134\n",
      "Epoch [2/2], Step [200/600], Loss: 0.1032\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1905\n",
      "Epoch [2/2], Step [400/600], Loss: 0.1657\n",
      "Epoch [2/2], Step [500/600], Loss: 0.1897\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1556\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "# Define Hyperparams\n",
    "input_size = 784 # img_size = (28,28)\n",
    "num_classes = 10 \n",
    "num_epochs = 10 \n",
    "batch_size = 100 \n",
    "lr = 1e-3\n",
    "\n",
    "# Define Model\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, num_classes):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, 500)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(500, num_classes)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out\n",
    "\n",
    "# Instantiate Model and move to GPU\n",
    "net = Net(input_size, num_classes)\n",
    "\n",
    "# Define Loss Function/Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "fit(model=net, loss_fn=loss_function, optimizer=optimizer, train_loader=train_loader, batch_size=batch_size, num_epochs=2, input_size=input_size)\n",
    "test_accuracy(model=net, test_loader=test_loader, input_size=input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
