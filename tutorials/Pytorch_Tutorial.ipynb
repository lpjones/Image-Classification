{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJwbj8T7HTAI"
   },
   "source": [
    "# Intro to Pytorch\n",
    "Pytorch is a popular Python framework used by many deep learning\n",
    "practitioners in both industry and academia. Pytorch provides all the \n",
    "tools necessary to load data, train a model, and accelerate matrix\n",
    "computations on GPUs without needing to know specific knowledge\n",
    "of the learning algorithms involved. Pytorch was designed with ease of use\n",
    "in mind (like Python), so it is the perfect first deep learning language to work with.\n",
    "\n",
    "In order to use Pytorch in your Python code you can import it with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEujHoMSHMmX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1d40f3c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGIr-c0sHSLW"
   },
   "source": [
    "The core data structure in Pytorch is the **tensor**. For those unfamiliar, tensors are essentially matrices expanded to higher dimensions. Tensors are difficult to visualize at times due to the fact they can scale to higher dimensions. Take a look at the following diagram to get a better visual of tensors themselves.\n",
    "\n",
    "\n",
    "![alt text](https://www.cc.gatech.edu/~san37/img/dl/tensor.png)\n",
    "\n",
    " The idea behind tensors is you can index into lower dimensions from higher dimensions. As an example, one of the entries in a 6-D tensor is a 5-D tensor. Indexing into the 5-D tensor gives a 4-D tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 3, 2])\n",
      "torch.Size([4, 3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Make a 6-D tensor with random entries\n",
    "t = torch.randn(6, 5, 4, 3, 3, 2)\n",
    "\n",
    "# Indexing into 6-D gives a 5-D tensor\n",
    "print(t[0].shape)\n",
    "\n",
    "# Indexing into 5-D tensor gives 4-D tensor\n",
    "print(t[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owtyC2_6I7V3"
   },
   "source": [
    "## Familiarizing yourself with Pytorch\n",
    "Lets learn how to initialize Pytorch tensors in different ways before we get into the more complicated parts of Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t-bXTgP8IzSg",
    "outputId": "781b737e-1d74-4446-8db2-03fc58b44e9b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([5, 3])\n",
      "tensor([[1.4013e-45, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3822e-44, 0.0000e+00],\n",
      "        [2.3822e-44, 7.0065e-45, 1.4013e-45],\n",
      "        [0.0000e+00, 3.2422e-36, 3.0728e-41],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "torch.float32\n",
      "torch.Size([5, 3])\n",
      "tensor([[ 0.5781, -0.2402, -0.2009],\n",
      "        [-0.4334, -2.2371, -1.2284],\n",
      "        [ 0.6461, -0.6719, -0.7497],\n",
      "        [ 0.5004, -0.8521,  0.2625],\n",
      "        [ 1.1861,  0.2846, -0.4051]])\n",
      "torch.float32\n",
      "torch.Size([10, 3, 4])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "torch.int64\n",
      "tensor([[3, 4],\n",
      "        [5, 7]])\n",
      "torch.float32\n",
      "tensor([[[ 0.0790,  0.4075,  1.0390, -0.4375],\n",
      "         [ 0.9912,  0.4096,  1.0504, -0.6872],\n",
      "         [-0.9539,  0.7271, -0.7682, -1.3224]],\n",
      "\n",
      "        [[ 0.4805, -0.4305,  0.0365,  0.1269],\n",
      "         [ 0.5966, -0.2306,  0.4388,  0.3811],\n",
      "         [ 1.7162, -0.0039, -2.0564, -1.5919]],\n",
      "\n",
      "        [[-0.2452,  0.3028,  1.1842,  0.6712],\n",
      "         [-1.0036,  0.4756,  0.8377,  0.8374],\n",
      "         [-0.7942, -0.3622,  2.1609,  0.3148]],\n",
      "\n",
      "        [[ 3.0974,  0.0121,  0.8032, -0.6962],\n",
      "         [-1.0645,  0.2384,  0.0132, -0.4237],\n",
      "         [-0.0926, -0.8519, -0.0543, -0.8025]],\n",
      "\n",
      "        [[-1.1104,  0.2151, -0.3594,  0.5785],\n",
      "         [ 0.7806,  1.2963, -0.7942, -1.1428],\n",
      "         [ 0.0493,  0.4103,  0.0306, -0.0628]],\n",
      "\n",
      "        [[-0.5699, -1.2050,  0.2543, -0.0637],\n",
      "         [ 0.1894,  0.8253, -0.7014,  0.7878],\n",
      "         [-1.1025,  0.1270,  1.8245, -0.1446]],\n",
      "\n",
      "        [[-1.7125,  0.3617, -0.4625, -2.1531],\n",
      "         [ 0.5370,  0.5816, -1.3250, -1.3147],\n",
      "         [-0.5248, -1.3042, -1.0938, -1.7034]],\n",
      "\n",
      "        [[-0.6730,  0.5053,  1.4977, -0.5454],\n",
      "         [-1.3346,  0.4745,  0.4844,  0.4344],\n",
      "         [-0.7335,  0.4530,  0.3246, -1.3075]],\n",
      "\n",
      "        [[-0.6406, -0.4501,  0.7729,  1.2818],\n",
      "         [-0.4817,  1.2247, -0.4375,  0.3720],\n",
      "         [-0.0245,  1.8515,  1.3344, -0.5670]],\n",
      "\n",
      "        [[ 1.3070, -0.9284, -0.8253,  0.5485],\n",
      "         [ 0.2729, -0.7707, -0.6243,  0.1975],\n",
      "         [ 0.4855,  0.1807, -0.1738, -0.9380]]])\n"
     ]
    }
   ],
   "source": [
    "# Create an empty tensor with dimensions 5x3 (5 rows, 3 columns)\n",
    "x = torch.empty(5, 3)\n",
    "print(x.dtype)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "# Create a tensor of size 5x3 with entries drawn from the normal distribution\n",
    "y = torch.randn(5, 3)\n",
    "print(y.dtype)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "# Create a tensor of size 10x3x4 with floats that are zeros\n",
    "z = torch.zeros(10, 3, 4, dtype=torch.float)\n",
    "print(z.dtype)\n",
    "print(z.shape)\n",
    "print(z)\n",
    "\n",
    "# Create a tensor from a Python list\n",
    "a = torch.tensor([[3, 4], [5, 7]])\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "\n",
    "# Create a tensor with the same size as another tensor\n",
    "b = torch.randn_like(z)\n",
    "print(b.dtype)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GADnDnYnKHYu"
   },
   "source": [
    "## Basic Operations on Tensors\n",
    "The main purpose of Pytorch is to do fast tensor operations. The following lines of code demonstrates how to do basic operations work with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7h9gz5uAOW9o",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8586,  1.4652, -3.4238, -1.2892, -0.5180],\n",
      "        [ 0.2684,  0.3453,  0.3403,  1.2260, -0.2613],\n",
      "        [ 2.5284,  0.2674,  0.3439,  0.5047,  2.9751],\n",
      "        [ 0.2573,  0.3008,  2.2651, -2.4236,  0.1482],\n",
      "        [ 2.4482, -0.5288, -0.4339, -0.8041, -0.8903],\n",
      "        [-2.5760, -0.5083,  1.0593, -1.7562, -0.8925],\n",
      "        [-1.8932, -0.4112,  1.2690, -0.4528, -1.9837],\n",
      "        [ 0.5995,  0.7161, -0.5758, -0.4285, -0.7273],\n",
      "        [ 2.8721, -0.8013, -0.8027,  2.7529,  1.7555],\n",
      "        [-0.4017,  0.6079,  1.2634,  0.4352, -1.9197]])\n",
      "tensor([[-0.8586,  1.4652, -3.4238, -1.2892, -0.5180],\n",
      "        [ 0.2684,  0.3453,  0.3403,  1.2260, -0.2613],\n",
      "        [ 2.5284,  0.2674,  0.3439,  0.5047,  2.9751],\n",
      "        [ 0.2573,  0.3008,  2.2651, -2.4236,  0.1482],\n",
      "        [ 2.4482, -0.5288, -0.4339, -0.8041, -0.8903],\n",
      "        [-2.5760, -0.5083,  1.0593, -1.7562, -0.8925],\n",
      "        [-1.8932, -0.4112,  1.2690, -0.4528, -1.9837],\n",
      "        [ 0.5995,  0.7161, -0.5758, -0.4285, -0.7273],\n",
      "        [ 2.8721, -0.8013, -0.8027,  2.7529,  1.7555],\n",
      "        [-0.4017,  0.6079,  1.2634,  0.4352, -1.9197]])\n",
      "tensor([[-0.8586,  1.4652, -3.4238, -1.2892, -0.5180],\n",
      "        [ 0.2684,  0.3453,  0.3403,  1.2260, -0.2613],\n",
      "        [ 2.5284,  0.2674,  0.3439,  0.5047,  2.9751],\n",
      "        [ 0.2573,  0.3008,  2.2651, -2.4236,  0.1482],\n",
      "        [ 2.4482, -0.5288, -0.4339, -0.8041, -0.8903],\n",
      "        [-2.5760, -0.5083,  1.0593, -1.7562, -0.8925],\n",
      "        [-1.8932, -0.4112,  1.2690, -0.4528, -1.9837],\n",
      "        [ 0.5995,  0.7161, -0.5758, -0.4285, -0.7273],\n",
      "        [ 2.8721, -0.8013, -0.8027,  2.7529,  1.7555],\n",
      "        [-0.4017,  0.6079,  1.2634,  0.4352, -1.9197]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 5)\n",
    "y = torch.randn(10, 5)\n",
    "\n",
    "# Different ways to add\n",
    "print(x + y)\n",
    "# Add and store result in new tensor\n",
    "result = torch.empty(10, 5)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# Add and store result in y, use _ to \"in place\" operations\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qAytvxPPPHR"
   },
   "source": [
    "## Tensor Indexing\n",
    "Tensor indexing can be fairly complicated if you are unfamiliar with it. This section goes through some standard ways to index slice and interact with the data with a tensor. We will use the example of RGB images in order to demonstrate the utility behind some of these operations. The biggest thing is understanding how the colon operator works. Essentially, the colon operator selects every index in the specified range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "tmd01iutPmVo",
    "outputId": "509775f5-7f2f-428a-e0f6-cf3c9a597958",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(187.5496)\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 3, 3])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Create a random RGB image\n",
    "rand_im = (torch.randn(32, 36, 3) + 1) * 128 \n",
    "\n",
    "# Print the first entry in the red channel\n",
    "print(rand_im[0, 0, 0]) \n",
    "\n",
    "# Get the first row for each channel\n",
    "print(rand_im[:, 0].shape)\n",
    "\n",
    "# Get the 4th, 5th, and 6th row for each channel\n",
    "print(rand_im[:, 4:7].shape)\n",
    "\n",
    "# Get the first row for the red channel\n",
    "print(rand_im[:, 0, 0].shape)\n",
    "\n",
    "# Get the 4th, 5th, and 6th row for each channel\n",
    "# Split Image into invidual color channels \n",
    "red = rand_im[:, :, 0]\n",
    "green = rand_im[:, :, 1]\n",
    "blue = rand_im[:, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFgvUxTRIun"
   },
   "source": [
    "This was a very brief overview of how to slice and index using the colon operator on tensors. I highly recommend checking out this more [in depth tutorial.](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html). While the tutorial is for numpy, the Pytorch developers have ported the same operations over to torch tensors. \n",
    "\n",
    "## Artificially Modifying Tensor Shapes/Indexing\n",
    "There are many operations in Pytorch that can be used to modify the shape of tensors without actually changing the underlying data. This means that you can easily change tensor shapes in Pytorch without the operations taking a long time, because there is no actual memory movement. \n",
    "\n",
    "### View\n",
    "The $\\texttt{view()}$ function allows one to artificially change the shape of a tensor without moving any data. This makes it convenient for any times you need to quickly change the shape of a tensor, for example to match an expected input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20]) torch.Size([10, 2]) torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 4)\n",
    "a = x.view(20)\n",
    "b = x.view(10, 2)\n",
    "c = x.view(-1, 10) # infer the dimensions of the new view with -1\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute\n",
    "Permute is used to swap the axes of a tensor. Again, none of the data is actually moved, the axes are simply swapped. Say for example, you are using a convolutional layer that expects $\\texttt{NCHW}$ and your current tensor has a shape of $\\texttt{NHWC}$. You would then want to swap $C$ (the channel dimension) from the second to last axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 3])\n",
      "torch.Size([32, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 28, 28, 3)\n",
    "\n",
    "print(x.shape)\n",
    "# NHWC -> NCHW\n",
    "x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Memory Control\n",
    "While Pytorch doesn't expose a large amount of control over the underlying memory of a tensor, Pytorch does give the user some power over this matter. These operations are important to know as they can affect the network's ability to train. It is important that your memory stays valid for the operations you use. \n",
    "\n",
    "### Contiguous\n",
    "Often times, after modifying a tensor with a combination of $\\texttt{view}, \\texttt{permute}$ and other operations, your data will no longer be indexed correctly. In this case, you will want specifically reorder the underlying data and ensure the data is arranged contiguously for the indexing you've specified. If you don't do this, it could lead to accuracy issues when training. Because contiguous copies data to a new tensor it can be a potentially very expensive operation, so try to only use it when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "Failed view() call!\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(200)\n",
    "print(x[0])\n",
    "x = x.view(5, 5, 2, 2, 2)\n",
    "x = x.permute(3, 4, 1, 2, 0)\n",
    "x = x.view(4, 5, 2, 5)\n",
    "x = x.permute(2, 3, 0, 1)\n",
    "\n",
    "# Fail to call contiguous\n",
    "try:\n",
    "    x = x.view(200)\n",
    "except Exception as e:\n",
    "    print(\"Failed view() call!\")\n",
    "    print(str(e))\n",
    "\n",
    "# Use Contiguous to view to original shape\n",
    "x = x.contiguous()\n",
    "x = x.view(200)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "Reshape is similar to view or permute. You can use it to change the shape or swap the axes of a tensor. However, it will explicitly rearrange the underlying data and stores it in a new tensor. You can think of reshape as wrapping the view, permute, contiguous pattern into one function call. This can be more convenient and readable, but your performance can suffer greatly if reshape is called too many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(200)\n",
    "print(x[0])\n",
    "x = x.reshape(2, 5, 4, 5)\n",
    "x = x.reshape(200)\n",
    "print(x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSCAkAW8R96S"
   },
   "source": [
    "## Combining Tensors\n",
    "There are many situations where you will need to concatenate tensors together as part of an algorithm (concatenatation is a very common operation for tensors in deep learning). Below you will learn all the ways in which you can combine tensors together.\n",
    "\n",
    "### Stacking/Concatenating\n",
    "Concatenating two tensors will combine them along a specific dimension. Stacking on the other hand will combine the tensors across a new dimension. The example below illustrates the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XMgXsPR6Q50n",
    "outputId": "216c69ef-acbf-4dec-fba3-cd87af8946e0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 5)\n",
    "a_cat = torch.cat((a, a), dim=0)\n",
    "a_stack = torch.stack((a, a), dim=0)\n",
    "print(a_cat.shape)\n",
    "print(a_stack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze/Unsqueeze\n",
    "These functions are used to either add a dimension of size 1 at a certain axis, or remove a dimension of size 1. These will be useful for again, making sure tensor shapes are of the correct size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 3, 1])\n",
      "torch.Size([3, 4, 3])\n",
      "torch.Size([3, 1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4, 3, 1)\n",
    "\n",
    "# Remove the last dimension\n",
    "print(a.shape)\n",
    "a = a.squeeze(3)\n",
    "print(a.shape)\n",
    "\n",
    "# Unsqueeze at 1st dimension\n",
    "a = a.unsqueeze(1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoybDwrOSxxi"
   },
   "source": [
    "## GPU Accelerating Torch Operations\n",
    "Pytorch is so popular not just because it provides easy to use tools to develop deep learning models, but also because it provides GPU acceleration for tensor operations. Thanks to support from different sources, we do have access to GPUs in order to train our models. The following code snippet shows how you can use CUDA (Nvidia's proprietary GPU programming language) to accelerate your Pytorch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8xS9T97SiAt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and assign device\n",
    "# to available GPU\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "\n",
    "# Create a tensor and port it to GPU\n",
    "x = torch.randn(5, 5)\n",
    "x = x.to(device)\n",
    "\n",
    "# Create a tensor directly on the GPU\n",
    "y = torch.randn(5, 5, device=device)\n",
    "\n",
    "z = x + y\n",
    "\n",
    "# Operating on two GPU tensors keeps all computation on the GPU\n",
    "print(z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFKuI_dzUO2U"
   },
   "source": [
    "You will likely have very little experience programming GPU's unless you have taken a graphics course or one of the specialized GPGPU courses. There are some important rules to remember when developing on GPU's.\n",
    "\n",
    "\n",
    "*   Minimize memory transfers from CPU to GPU (create tensors directly on GPU)\n",
    "*   Write device agnostic code (Code should run on GPU if available, otherwise use CPU)\n",
    "\n",
    "\n",
    "You don't need to worry about knowing everything about a GPU when using Pytorch. However, you should at the very least follow these two rules as it will significantly increase your quality of life when developing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "These are just some of the operations available to you in Pytorch. I have provided a cursory understanding of each function for your convenience, however I would encourage you to take a look at the [official Pytorch docs](https://pytorch.org/docs/stable/index.html) if you have any confusion. There are also a plethora of other functions that you can take advantage of and read about."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab3_Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
